"""
Gemini ê¸°ë°˜ ì†Œë¹„ ë¶„ì„ / ì ˆì•½ ê°€ì´ë“œ LLM ì„œë¹„ìŠ¤
ìµœì í™” ë²„ì „: í”„ë¡¬í”„íŠ¸ ë‹¨ì¶•, ìºì‹±, í† í° ì œí•œ ì ìš©
"""

from fastapi import FastAPI, HTTPException
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import StreamingResponse
from pydantic import BaseModel
from typing import Optional, List, Dict
import google.generativeai as genai
import os
import logging
import hashlib
import json
import time
from functools import lru_cache

# ë¡œê¹… ì„¤ì •
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

app = FastAPI(
    title="Caffeine ì†Œë¹„ ë¶„ì„ AI",
    description="Google Gemini ê¸°ë°˜ ì†Œë¹„ íŒ¨í„´ ë¶„ì„ ë° ì ˆì•½ ê°€ì´ë“œ ì„œë¹„ìŠ¤ (ìµœì í™”)",
    version="2.0.0"
)

# CORS ì„¤ì •
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

# ============================================================
# ìºì‹œ ì„¤ì • (ë©”ëª¨ë¦¬ ê¸°ë°˜)
# ============================================================
response_cache: Dict[str, tuple] = {}  # {hash: (response, timestamp)}
CACHE_TTL = 300  # 5ë¶„ ìºì‹œ

def get_cache_key(prompt: str) -> str:
    """í”„ë¡¬í”„íŠ¸ í•´ì‹œ ìƒì„±"""
    return hashlib.md5(prompt.encode()).hexdigest()

def get_cached_response(prompt: str) -> Optional[str]:
    """ìºì‹œëœ ì‘ë‹µ ì¡°íšŒ"""
    key = get_cache_key(prompt)
    if key in response_cache:
        response, timestamp = response_cache[key]
        if time.time() - timestamp < CACHE_TTL:
            logger.info("âœ… ìºì‹œ íˆíŠ¸!")
            return response
        else:
            del response_cache[key]  # ë§Œë£Œëœ ìºì‹œ ì‚­ì œ
    return None

def set_cached_response(prompt: str, response: str):
    """ì‘ë‹µ ìºì‹œ ì €ì¥"""
    key = get_cache_key(prompt)
    response_cache[key] = (response, time.time())
    # ìºì‹œ í¬ê¸° ì œí•œ (ìµœëŒ€ 100ê°œ)
    if len(response_cache) > 100:
        oldest_key = min(response_cache, key=lambda k: response_cache[k][1])
        del response_cache[oldest_key]

# ============================================================
# Gemini API ì„¤ì •
# ============================================================
# docker-composeì—ì„œ GEMINI_API_KEYë¡œ ì „ë‹¬ë¨ (.envì˜ gemini_keyì—ì„œ)
GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") or os.getenv("gemini_key", "")
if not GEMINI_API_KEY:
    logger.warning("âš ï¸ GEMINI_API_KEYê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!")
else:
    logger.info(f"âœ… API Key ë¡œë“œë¨: {GEMINI_API_KEY[:10]}...")
genai.configure(api_key=GEMINI_API_KEY)

# ëª¨ë¸ ì´ˆê¸°í™” (ìµœì í™”: max_output_tokens ì œí•œ)
try:
    generation_config = {
        "max_output_tokens": 200,  # ë§¤ìš´ë§› ë‹µë³€ì„ ìœ„í•´ í† í° ì¦ê°€
        "temperature": 0.9,        # ë” ì¬ë¯¸ìˆëŠ” ë‹µë³€
    }
    model = genai.GenerativeModel(
        'gemini-2.0-flash-exp',
        generation_config=generation_config
    )
    logger.info("âœ… Gemini ëª¨ë¸ ì´ˆê¸°í™” ì„±ê³µ (gemini-2.0-flash-exp, max_tokens=200)")
except Exception as e:
    logger.error(f"âŒ Gemini ëª¨ë¸ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
    model = None


# ============================================================
# í”„ë¡¬í”„íŠ¸ (ë§¤ìš´ë§› AI + ê±°ë˜ ë‚´ì—­ ìƒì„¸)
# ============================================================

def get_transaction_prompt(merchant: str, amount: int, category: str, 
                          budget_pct: float, category_count: int, category_spent: int, status: str) -> str:
    """ê±°ë˜ í‰ê°€ìš© ë§¤ìš´ë§› í”„ë¡¬í”„íŠ¸"""
    return f"""ë‹¹ì‹ ì€ 'ì ê¹ë§ŒAI', íŒ©íŠ¸í­í–‰ ì¬ë¬´ íŠ¸ë ˆì´ë„ˆì•¼.

[ë°©ê¸ˆ ê±°ë˜]
- {merchant}ì—ì„œ {amount:,}ì› ì”€
- ì¹´í…Œê³ ë¦¬: {category} (ì´ë²ˆ ë‹¬ {category_count}ë²ˆì§¸, ì´ {category_spent:,}ì›)
- ì˜ˆì‚° {budget_pct:.0f}% ì‚¬ìš©, ìƒíƒœ: {status}

[ê·œì¹™]
1. ë°˜ë§ë¡œ 3ë¬¸ì¥ ì´ë‚´
2. ì´ëª¨ì§€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
3. ê³¼ì†Œë¹„ë©´ ë¹„ê¼¬ê³  í’ì, ì˜í–ˆìœ¼ë©´ ê²©í•˜ê²Œ ì¹­ì°¬
4. êµ¬ì²´ì  ìˆ«ì ì–¸ê¸‰

ì˜ˆì‹œ:
- ë°˜ë³µ: "ì™€ í•œë‹¬ì— {merchant} {category_count}ë²ˆì´ë‚˜ ê°„ë‹¤ê³ ? ë„ˆ ë•ë¶„ì— {merchant} ì´ë²ˆë‹¬ ì†Œê³ ê¸° ë¨¹ìŒ"
- ê³¼ì†Œë¹„: "{category}ì— {category_spent:,}ì› ì“°ëŠ” ê±° ì‹¤í™”? ì •ì‹  ëª»ì°¨ë¦¬ì§€?"
- ì ˆì•½: "ë“œë””ì–´ ì •ì‹ ì°¨ë ¸êµ¬ë‚˜! ì˜í•˜ê³  ìˆì–´. ê±´ë¬¼ì£¼ ë˜ë©´ ë‚˜ ìŠì§€ë§ê³ !"

ë°”ë¡œ ì¡°ì–¸í•´:"""


def get_chat_prompt(message: str, budget_pct: float, remaining: int, 
                   tx_count: int, top_category: str, category_summary: str, recent_tx: str) -> str:
    """ì±—ë´‡ìš© ë§¤ìš´ë§› í”„ë¡¬í”„íŠ¸ (ê±°ë˜ ë‚´ì—­ í¬í•¨)"""
    return f"""ë‹¹ì‹ ì€ 'ì ê¹ë§ŒAI', íŒ©íŠ¸í­í–‰í•˜ë©° ëˆ ì•„ë¼ê²Œ ë§Œë“œëŠ” ì¬ë¬´ íŠ¸ë ˆì´ë„ˆì•¼.

[ì‚¬ìš©ì ì¬ì • í˜„í™©]
- ì˜ˆì‚° ì‚¬ìš©: {budget_pct:.0f}% (ë‚¨ì€ ëˆ: {remaining:,}ì›)
- ì´ë²ˆ ë‹¬ ê±°ë˜: {tx_count}íšŒ

[ì¹´í…Œê³ ë¦¬ë³„ ì§€ì¶œ]
{category_summary if category_summary else "ì•„ì§ ê±°ë˜ ì—†ìŒ"}

[ìµœê·¼ ê±°ë˜]
{recent_tx if recent_tx else "ì—†ìŒ"}

[ê·œì¹™]
1. ë°˜ë§ë¡œ 3ë¬¸ì¥ ì´ë‚´
2. ì´ëª¨ì§€ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€
3. êµ¬ì²´ì  ìˆ«ìì™€ ê±°ë˜ ë‚´ì—­ ì–¸ê¸‰í•´ì„œ ë‹µë³€
4. ë¹„ê¼¬ê³  í’ìí•˜ë˜ ë„ì›€ë˜ê²Œ

[ì‚¬ìš©ì ì§ˆë¬¸]
{message}

ë°”ë¡œ ë‹µë³€í•´:"""



# ============================================================
# API ì—”ë“œí¬ì¸íŠ¸
# ============================================================

@app.get("/")
def health_check():
    """í—¬ìŠ¤ ì²´í¬"""
    return {
        "status": "ok",
        "service": "Caffeine ì†Œë¹„ ë¶„ì„ AI (ìµœì í™”)",
        "model": "gemini-2.0-flash-exp",
        "model_loaded": model is not None,
        "cache_size": len(response_cache)
    }


@app.post("/evaluate")
async def evaluate_transaction(request: dict):
    """í†µí•© AI ì—”ë“œí¬ì¸íŠ¸ - ê±°ë˜ í‰ê°€ ë° ì±—ë´‡ ëŒ€í™” (ìµœì í™”)"""
    if model is None:
        raise HTTPException(status_code=503, detail="Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    start_time = time.time()
    
    try:
        transaction = request.get("transaction", {})
        message = request.get("message", "")
        budget = request.get("budget", 1000000)
        spending_history = request.get("spending_history", {})
        
        # ë””ë²„ê¹…: ë°›ì€ ë°ì´í„° ë¡œê¹…
        logger.info(f"ğŸ“Š ë°›ì€ ë°ì´í„° - ì˜ˆì‚°: {budget:,}ì›, ì§€ì¶œ: {spending_history.get('total', 0):,}ì›, ê±°ë˜ìˆ˜: {spending_history.get('transaction_count', 0)}íšŒ")
        logger.info(f"ğŸ“Š ì¹´í…Œê³ ë¦¬: {list(spending_history.get('category_breakdown', {}).keys())[:3]}")
        logger.info(f"ğŸ“Š ìµœê·¼ê±°ë˜: {spending_history.get('recent_transactions', 'ì—†ìŒ')[:50]}...")
        
        total_spent = spending_history.get("total", 0)
        budget_percentage = (total_spent / budget * 100) if budget > 0 else 0
        remaining_budget = max(0, budget - total_spent)
        
        # ì¬ì • ìƒíƒœ íŒë‹¨
        if budget_percentage > 100:
            status = "íŒŒì‚°ì§ì „"
        elif budget_percentage > 80:
            status = "ìœ„í—˜"
        elif budget_percentage > 50:
            status = "ë³´í†µ"
        else:
            status = "ì—¬ìœ "
        
        # ê±°ë˜ í‰ê°€ì¸ ê²½ìš°
        if transaction and transaction.get("merchant_name"):
            merchant = transaction.get("merchant_name", "?")
            amount = transaction.get("amount", 0)
            category = transaction.get("category", "ê¸°íƒ€")
            category_count = spending_history.get("category_count", 1)
            category_spent = spending_history.get("category_total", 0)
            
            prompt = get_transaction_prompt(
                merchant, amount, category, 
                budget_percentage, category_count, category_spent, status
            )
            req_type = "transaction"
            logger.info(f"ğŸ“ ê±°ë˜ í‰ê°€: {merchant} {amount:,}ì›")
        
        # ì¼ë°˜ ëŒ€í™”ì¸ ê²½ìš°
        elif message:
            tx_count = spending_history.get("transaction_count", 0)
            category_breakdown = spending_history.get("category_breakdown", {})
            recent_transactions = spending_history.get("recent_transactions", "")
            
            # TOP ì¹´í…Œê³ ë¦¬ ì¶”ì¶œ
            top_category = "ì—†ìŒ"
            category_summary = ""
            if category_breakdown:
                # ì¹´í…Œê³ ë¦¬ë³„ ìš”ì•½ ìƒì„±
                sorted_cats = sorted(category_breakdown.items(), 
                                    key=lambda x: x[1].get('total', 0), reverse=True)[:3]
                category_summary = "\n".join([
                    f"- {cat}: {info.get('count', 0)}íšŒ, {info.get('total', 0):,}ì›" 
                    for cat, info in sorted_cats
                ])
                if sorted_cats:
                    top_cat = sorted_cats[0]
                    top_category = f"{top_cat[0]}({top_cat[1].get('count', 0)}íšŒ)"
            
            prompt = get_chat_prompt(
                message, budget_percentage, remaining_budget, 
                tx_count, top_category, category_summary, recent_transactions
            )
            req_type = "chat"
            logger.info(f"ğŸ’¬ ì±—ë´‡ ëŒ€í™”: {message[:20]}...")
        
        else:
            raise HTTPException(status_code=400, detail="transaction ë˜ëŠ” message í•„ìˆ˜")
        
        # ìºì‹œ í™•ì¸
        cached = get_cached_response(prompt)
        if cached:
            elapsed = time.time() - start_time
            return {
                "status": "success",
                "message": cached,
                "model": "gemini-2.0-flash-exp",
                "type": req_type,
                "cached": True,
                "elapsed_ms": int(elapsed * 1000)
            }
        
        # Gemini API í˜¸ì¶œ
        response = model.generate_content(prompt)
        result = response.text.strip()
        
        # ìºì‹œ ì €ì¥
        set_cached_response(prompt, result)
        
        elapsed = time.time() - start_time
        logger.info(f"âš¡ ì‘ë‹µ ì™„ë£Œ: {int(elapsed * 1000)}ms")
        
        return {
            "status": "success",
            "message": result,
            "model": "gemini-2.0-flash-exp",
            "type": req_type,
            "cached": False,
            "elapsed_ms": int(elapsed * 1000)
        }
        
    except Exception as e:
        logger.error(f"âŒ AI ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=f"ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {str(e)}")


@app.post("/evaluate/stream")
async def evaluate_stream(request: dict):
    """ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µ (UX ê°œì„ )"""
    if model is None:
        raise HTTPException(status_code=503, detail="Gemini ëª¨ë¸ì´ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤")
    
    try:
        message = request.get("message", "")
        budget = request.get("budget", 1000000)
        spending_history = request.get("spending_history", {})
        
        total_spent = spending_history.get("total", 0)
        budget_percentage = (total_spent / budget * 100) if budget > 0 else 0
        remaining_budget = max(0, budget - total_spent)
        tx_count = spending_history.get("transaction_count", 0)
        
        prompt = get_chat_prompt(message, budget_percentage, remaining_budget, tx_count, "")
        
        async def generate():
            response = model.generate_content(prompt, stream=True)
            for chunk in response:
                if chunk.text:
                    yield f"data: {json.dumps({'text': chunk.text})}\n\n"
            yield "data: [DONE]\n\n"
        
        return StreamingResponse(generate(), media_type="text/event-stream")
        
    except Exception as e:
        logger.error(f"ìŠ¤íŠ¸ë¦¬ë° ì‹¤íŒ¨: {e}")
        raise HTTPException(status_code=500, detail=str(e))


@app.get("/cache/stats")
def cache_stats():
    """ìºì‹œ í†µê³„"""
    return {
        "cache_size": len(response_cache),
        "cache_keys": list(response_cache.keys())[:10],  # ìµœê·¼ 10ê°œë§Œ
        "ttl_seconds": CACHE_TTL
    }


@app.delete("/cache/clear")
def clear_cache():
    """ìºì‹œ ì´ˆê¸°í™”"""
    response_cache.clear()
    return {"status": "ok", "message": "ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤"}
